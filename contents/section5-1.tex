%% SECTION 5.1 %%
\section{Empirical Risk Minimization}

As in chapters \ref{ch: binary classification} and \ref{ch: VC theory}, we want to control the estimation error of the empirical risk minimizer $\hat{f}$. Using the same arguments as before, by Lemma \ref{lem: bound on estimation error}, we can bound this error somewhat crudely as follows:
\[
    L(\hat{f}) - L(\bar{f}) \leq 2 \sup_{f \in \mathcal{F}} \abs*{\hat{L}_n(f) - L(f)}.
\]
The function
\[
    (z_1, \dots, z_n) \mapsto \sup_{f \in \mathcal{F}} \abs*{\hat{L}_n(f) - L(f)} = \sup_{f \in \mathcal{F}} \abs{\frac{1}{n} \sum_{i=1}^n l(f(x_i), y_i) - L(f)}
\]
satisfies the bounded differences condition for constants $c_i = \frac{1}{n}$, since the loss function $l$ is assumed to be bounded, i.e., $0 \leq l \leq 1$. Thus, by the bounded differences inequality (Theorem \ref{thm: bounded differences inequality}), we obtain
\[
    \Prob\left(\sup_{f \in \mathcal{F}} \abs*{\hat{L}_n(f) - L(f)} - \Exp[\sup_{f \in \mathcal{F}} \abs*{\hat L_n(f) - L(f)}] \geq t\right) \leq \e^{-2nt^2}.
\]
In particular,
\[
    \Prob\left(\sup_{f \in \mathcal{F}} \abs*{\hat{L}_n(f) - L(f)} - \Exp[\sup_{f \in \mathcal{F}} \abs*{\hat{L}_n(f) - L(f)}] < t\right) \geq 1 - \e^{-2nt^2},
\]
and by evaluating this inequality at $t = \sqrt{\log(\delta^{-1}) / 2n}$, we see that the inequality
\[
    \sup_{f \in \mathcal{F}} \abs*{\hat{L}_n(f) - L(f)} < {\color{blue} \Exp[\sup_{f \in \mathcal{F}} \abs*{\hat L_n(f) - L(f)}]} + \sqrt{\frac{\log(\delta^{-1})}{2n}}
\]
holds with probability at least $1 - \delta$. Therefore, as in Chapter \ref{ch: VC theory}, it remains to control the \textcolor{blue}{first term on the RHS}. Ideally, we can do so independently of the (unknown) distribution of $(X, Y)$.

Throughout this chapter, we will illustrate the main results on the following class of functions:

\begin{example}[Linear Functions]
\label{ex: linear functions f_a}
Let
\[
    B_p^d(1) = \{x \in \R^d \with \norm{x}_p \leq 1\}
\]
be the closed ball of radius $1$ in $\R^d$ with respect to the $p$-norm $\norm{x}_p = (\sum_{i=1}^d \abs{x_i}^p)^{\nicefrac{1}{p}}$. In particular, for $p = \infty$ we obtain the Chebyshev norm $\norm{x}_{\infty} = \max_{i=1, \dots, d} \abs{x_i}$. We set $\mathcal{X} = B_1^d(1)$, and let $\mathcal{F}$ be the class of linear functions
\[
    f_a \colon \mathcal{X} \to \R, \quad x \mapsto \langle a, x \rangle, \qquad a \in B_{\infty}^d(1).
\]
We recall H{\"o}lder's inequality, which states that, for $p, q \in [1, \infty]$ with\footnote{by convention, $\nicefrac{1}{\infty} = 0$} $\nicefrac{1}{p} + \nicefrac{1}{q} = 1$,
\[
    \sum_{i=1}^d \abs{x_i y_i} \leq \norm{x}_p \norm{y}_q, \quad x, y \in \R^d.
\]
By combining H{\"o}lder's inequality with the triangle inequality, we have
\[
    \abs{f_a(x)} = \abs{\langle a, x \rangle} \leq \sum_{i=1}^d \abs{a_i x_i} \leq \norm{a}_{\infty} \norm{x}_1 \leq 1, \quad x \in \mathcal{X}.
\]
Hence, for $a \in B_{\infty}^d(1)$, the linear function $f_a \in \mathcal{F}$ takes values in the interval $[-1, 1]$, i.e.,
\begin{equation}
\label{eq: family of linear functions f_a}
    \mathcal{F} = \set{f_a \colon B_1^d(1) \to [-1, 1], \; x \mapsto \langle a, x \rangle \with a \in B_{\infty}^d(1)}.
\end{equation}
\end{example}
