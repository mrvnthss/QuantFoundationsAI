%% SECTION 5.1 %%
\section{Empirical Risk Minimization}

As in chapters \ref{ch: binary classification} and \ref{ch: VC theory}, we want to control the estimation error of the empirical risk minimizer $\hat{f}$. Using the same arguments as before, by Lemma \ref{lem: bound on estimation error}, we can bound this error somewhat crudely as follows:
\[
    L(\hat{f}) - L(\bar{f}) \leq 2 \sup_{f \in \mathcal{F}} \abs*{\hat{L}_n(f) - L(f)}.
\]
The function
\[
    (z_1, \dots, z_n) \mapsto \sup_{f \in \mathcal{F}} \abs*{\hat{L}_n(f) - L(f)} = \sup_{f \in \mathcal{F}} \abs{\frac{1}{n} \sum_{i=1}^n l(f(x_i), y_i) - L(f)}
\]
satisfies the bounded differences condition for constants $c_i = \frac{1}{n}$, since the loss function $l$ is assumed to be bounded, i.e., $0 \leq l \leq 1$. Thus, by the bounded differences inequality (Theorem \ref{thm: bounded differences inequality}), we obtain
\[
    \Prob\left(\sup_{f \in \mathcal{F}} \abs*{\hat{L}_n(f) - L(f)} - \Exp[\sup_{f \in \mathcal{F}} \abs*{\hat L_n(f) - L(f)}] \geq t\right) \leq \e^{-2nt^2}.
\]
In particular,
\[
    \Prob\left(\sup_{f \in \mathcal{F}} \abs*{\hat{L}_n(f) - L(f)} - \Exp[\sup_{f \in \mathcal{F}} \abs*{\hat{L}_n(f) - L(f)}] < t\right) \geq 1 - \e^{-2nt^2},
\]
and by evaluating this inequality at $t = \sqrt{\log(\delta^{-1}) / 2n}$, we see that the inequality
\[
    \sup_{f \in \mathcal{F}} \abs*{\hat{L}_n(f) - L(f)} < \Exp[\sup_{f \in \mathcal{F}} \abs*{\hat L_n(f) - L(f)}] + \sqrt{\frac{\log(\delta^{-1})}{2n}}
\]
holds with probability at least $1 - \delta$. Therefore, as in Chapter \ref{ch: VC theory}, it remains to control the first term on the RHS. Ideally, we can do so independently of the (unknown) distribution of $(X, Y)$.
