%% SECTION 2.3 %%
\section{Empirical Classification}
\label{subsec: empirical classification}

In reality, we do \emph{not} know the true distribution of $(X, Y)$. Instead, we are often working with finite sets of data $\mathcal{D}_n = \set{(X_1, Y_1), \dots, (X_n, Y_n)}$ (e.g., a training set), where the $(X_i, Y_i)$ are (hypothetical) \emph{future} i.i.d. draws from $\P_{(X, Y)}$. Note that, if we build a classifier $h_n(X)$ based on this data, the classifier is random in two senses: it is a function of a random variable $X$, and it depends implicitly on the random data $\mathcal{D}_n$. Further, the error probability
\[
    L(h_n) = \P(h_n (X) \neq Y)
\]
is a \emph{random variable}, as it depends on the random data $\mathcal{D}_n$! Nevertheless, the excess risk
\[
    R(h_n) = L(h_n) - L^* \geq 0
\]
is always non-negative (in a deterministic sense, \emph{not} only almost surely). One approach of building a classifier based on the observed data $\mathcal{D}_n$ is the following: based on the observations $(X_1, Y_1), \dots, (X_n, Y_n)$, we estimate $\eta$ by $\eta_n$ and then construct the plug-in classifier $h_n = \indSet{\eta_n > \nicefrac{1}{2}}$. However, the (true) excess risk of $h_n$ is \emph{not} computable based on the data alone and we cannot use any results for upper bounds on the excess risk $R(h_n)$ discussed in Section \ref{subsec: plug-in decisions}. A naive attempt to solve this problem is the definition of \emph{empirical risk}, which essentially replaces the expected deviation of $h_n(X)$ from $Y$ by a simple average:

\begin{definition}
The \emph{empirical risk} of a classifier $h$ with respect to the data $\mathcal{D}_n$ is given by
\[
    \hat L_n(h) = \frac{1}{n} \sum_{i=1}^n \indSet{h(X_i) \neq Y_i}.
\]
\end{definition}

The empirical risk $\hat L_n(h)$ of a classifier $h$ is an \emph{unbiased estimator} of its true risk $L(h)$, i.e.,
\begin{equation}
\label{eq: bias of empirical risk}
    \highlightMath{
        \Ex{\hat L_n(h)} = \frac{1}{n} \sum_{i=1}^n \Ex{\indSet{h(X_i) \neq Y_i}} = \frac{1}{n} \sum_{i=1}^n \Pr{h(X_i) \neq Y_i} = \Pr{h(X) \neq Y} = L(h),
    }
\end{equation}
since $(X_i, Y_i)$ are i.i.d. draws from the distribution of $(X, Y)$.

Minimizing the empirical risk over all classifiers is useless since, for every hypothetical data set, we can always construct a classifier with \emph{no} empirical risk by just mimicking the data and classifying arbitrarily otherwise, i.e., the classifier
\[
    h_n(x) = \begin{cases}
        1, \quad &x = X_i \text{ for some } i = 1, \dots, n \text{ with } Y_i = 1 \\
        0, \quad &x = X_i \text{ for some } i = 1, \dots, n \text{ with } Y_i = 0 \\
        a, \quad &x \notin \set{X_1, \dots, X_n}
    \end{cases}
\]
with $a \in \set{0, 1}$ arbitrary, satisfies $\hat L_n(h_n) = 0$. Thus, we have to restrict ourselves to classifiers in a certain family $\mathcal{H}$.

\begin{definition}
The \emph{empirical risk minimizer} over $\mathcal{H}$ is any element $\hat h^{\text{erm}}$ of the set $\argmin_{h \in \mathcal{H}} \hat L_n(h)$, i.e., $\hat h^{\text{erm}} \in \mathcal{H}$ and $\hat L_n(\hat h^{\text{erm}}) = \min_{h \in \mathcal{H}} \hat L_n(h)$.
\end{definition}

For meaningful results, $\mathcal{H}$ should be much smaller than the set of all classifiers. On the other hand, $\mathcal{H}$ should not be \emph{too} small, because we want the empirical risk of $\hat h^{\text{erm}}$ to be close to the Bayes risk. Often, we will be satisfied with a classifier $\hat h$ that is reasonably close to the empirical risk minimizer $\hat h^{\text{erm}}$ in the sense that
\[
    \hat L_n(\hat h) \leq \min_{h \in \mathcal{H}} \hat L_n(h) + \varepsilon, \quad \text{for small } \varepsilon > 0.
\]
Given a family $\mathcal{H}$, let $\bar h$ be a classifier that somehow minimizes the \emph{true} risk, i.e.,
\[
    \highlightMath{
        \bar h \in \argmin_{h\in \mathcal{H}} L(h).
    }
\]
Note that it is impossible to construct such a classifier -- often called an \emph{oracle} -- from data alone. Nonetheless, we can attempt to find a classifier $\hat h$ that mimics the performance of the oracle $\bar h$ in the sense that we can hope to prove a bound of the form
\begin{equation}
    \label{eq: oracle inequality}
    L(\hat h) \leq L(\bar h) + \text{something small}.
\end{equation}
Such inequalities are often called \emph{oracle inequalities} as they involve the oracle $\bar h$. In \eqref{eq: oracle inequality}, we once again see the trade-off regarding the size of $\mathcal{H}$. If $\mathcal{H}$ is small, the performance of the oracle $\bar h$ is likely to suffer, while it might be possible to approximate $\bar h$ quite closely. If, on the other hand, $\mathcal{H}$ is quite large, the oracle $\bar h$ will be very powerful, but approximating $\bar h$ becomes statistically more difficult.

Ultimately\footnote{Remember that $L(\hat f)$ is a random variable!}, we want to prove tail bounds or bounds in expectation of the form
\[
    \Pr{L(\hat h) \leq L(\bar h) + \Delta_{n, \delta}(\mathcal{H})} \geq 1 - \delta,
\]
where $\Delta_{n, \delta}(\mathcal{H})$ is some function of $\mathcal{H}$ depending on our sample size $n$ and our desired level of confidence $\delta$.

Finally, note that we can decompose the excess risk $R(\hat h)$ of a classifier as follows:
\[
    \highlightMath{
        R(\hat h) = L(\hat h) - L^* = \underbrace{L(\hat h) - L(\bar h)}_{\substack{\text{estimation} \\ \text{error}}} + \underbrace{L(\bar h) - L^*}_{\substack{\text{approximation} \\ \text{error}}}
    }
\]
The second term is the \emph{approximation error} that is unavoidable once we fix a family of classifiers $\mathcal{H}$. Oracle inequalities provide a method to bound the first term, the \emph{estimation error}.
