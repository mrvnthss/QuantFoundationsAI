%% EXERCISE SET 4 %%
\section{Exercise Set 4}

% EXERCISE 11
\begin{exercise}
Let $\mathcal{F}$ be a class of functions $f \colon \mathcal{X} \to \R$, and let $X_1, \dots, X_n$ be i.i.d.\ random variables with values in $\mathcal{X}$. Further, let $\sigma_1, \dots, \sigma_n$ be i.i.d.\ $\Rad(\nicefrac{1}{2})$ random variables that are independent of $X_1, \dots, X_n$. Prove the \emph{desymmetrization inequality}:
\[
    \Exp\left[\sup_{f \in \mathcal{F}} \abs{\frac{1}{n} \sum_{i=1}^n \sigma_i (f(X_i) - \Exp[f(X_i)])}\right] \leq 2 \Exp\left[\sup_{f \in \mathcal{F}} \abs{\frac{1}{n} \sum_{i=1}^n (f(X_i) - \Exp[f(X_i)])}\right].
\]
\end{exercise}


% EXERCISE 12
\begin{exercise}
Let $\mathcal{F}$ be a class of functions $f \colon \mathcal{X} \to \R$, and let $X_1, \dots, X_n$ be i.i.d.\ random variables with values in $\mathcal{X}$. Further, let $\sigma_1, \dots, \sigma_n$ be i.i.d.\ $\Rad(\nicefrac{1}{2})$ random variables, and let $g_1, \dots, g_n$ be i.i.d.\ $\mathcal{N}(0, 1)$ random variables. Finally, assume that $X_1, \dots, X_n, \sigma_1, \dots, \sigma_n, g_1, \dots, g_n$ are independent.
\begin{enumerate}
    \item Prove $g_i \stackrel{d}{=} \sigma_i \abs{g_i}$.

    \item Show that $\Exp[\abs{g_i}] = \sqrt{2 / \pi}$.

    \item Derive the following Rademacher-Gaussian comparison inequality:
            \[
                \Exp\left[\sup_{f \in \mathcal{F}} \abs{\sum_{i=1}^n \sigma_i f(X_i)}\right] \leq \sqrt{\frac{\pi}{2}} \Exp\left[\sup_{f \in \mathcal{F}} \abs{\sum_{i=1}^n g_i f(X_i)}\right] .
            \]
\end{enumerate}
\end{exercise}


% EXERCISE 13
\begin{exercise}
Consider a neural network $\psi \colon \R^d \to \set{-1, 1}$ with one hidden layer made up of $k$ hidden neurons, i.e.,
\[
    \psi(x) = \phi\left(b^{[2]} + \sum_{k=1}^n w_k^{[2]} \sigma\left(b_k^{[1]} + \sum_{j=1}^d w_{k, j}^{[1]} x^{(j)} \right)\right), \quad x \in \R^d,
\]
where
\[
    \phi(z) = \begin{cases}
        1, \quad & z > 0 \\
        -1, \quad & z \leq 0
    \end{cases}
\]
is the threshold sigmoid, $b^{[2]}$ and $w_1^{[2]}, \dots, w_n^{[2]}$ are the bias and the weights of the output layer, respectively, $\sigma \colon \R \to [-1, 1]$ is an arbitrary sigmoid, and $b_k^{[1]}$ and $w_{k, 1}^{[1]}, \dots, w_{k, d}^{[1]}$ are the bias and weights of the $k$-th hidden neuron, respectively. Given a fixed training set $\mathcal{D}_k = \set{(X_1, Y_1), \dots, (X_n, Y_n)}$ consisting of as many samples $(X_i, Y_i) \in \R^d \times \set{-1, 1}$ as we have hidden neurons, where $X_i \neq X_l$ for $i \neq l$, prove that we can choose the biases and weights of $\psi$ such that
\[
    \psi(X_i) = Y_i, \quad i = 1, \dots, n.
\]
This phenomenon of perfectly predicting the training set is called \emph{overfitting}, which occurs when the neural network becomes too \say{rich}.
\end{exercise}
