%% CHAPTER 2 %%
\chapter{Binary Classification}
\label{ch: binary classification}

In the first chapter, we described the formal setting that will accompany us for the remainder of this course as we tackle various problems that machine learning aims to solve.

In this chapter, we will focus our attention on one of these problems, namely \emph{binary classification}. The decision to focus on \emph{binary} (rather than \emph{non-binary}) classification is based on several reasons. On the one hand, binary classification covers much of what we want to accomplish in practice, and the response variables are bounded. On the other hand, we avoid some of the nasty surprises of non-binary classification.

Let us first recall the setup of binary classification: the data we observe is a sequence $(X_1, Y_1), \dots, (X_n, Y_n)$ of $n$ \emph{feature-label pairs}, where each $(X_i, Y_i)$ is an independent draw from some (unknown) joint distribution $\P_{(X, Y)}$. The random variable $Y$ takes values in $\set{0, 1}$ and $X$ takes values in some \emph{feature space} $\mathcal{X}$, which in many use cases will be $\R^d$. Because $Y$ is supported on $\set{0, 1}$, the conditional random variable $Y \vert X$ follows a Bernoulli distribution, which we denote by $Y \vert X \sim \mathrm{Ber}(\eta(X))$, where
\[
    \highlightMath{
        \eta(X) = \Pr{Y = 1 \given X} = \Ex{Y \given X}
    }
\]
is the \emph{regression function}. For $x \in \mathcal{X}$, the expression\footnote{Note that $\eta(X) \colon \Omega \to [0, 1]$ is a random variable defined on some probability space $(\Omega, \mathcal{A}, \P)$, while $\eta \colon \mathcal{X} \to [0, 1]$ is a deterministic function.} $\eta(x) = \Pr{Y = 1 \given X = x}$ is often called the \emph{a posteriori probability} of $Y$ given $x$. 
