%% SECTION 1.1 %%
\section{Loss and Expected Error}

In the context of machine learning, the random variable $Y$ is generally some output/label resulting from or associated with a given input/observation $X$. In this setting, we often want to approximate or predict the value of $Y$ given the input $X$. That is, we want to find a measurable function
\[
    f \colon (\mathcal{X}, \mathcal{F}_{\mathcal{X}}) \to (\mathcal{Y}, \mathcal{F}_{\mathcal{Y}})
\]
mapping $X$ to $f(X)$ in such a way that, ideally, $f(X) = Y$. Not surprisingly, perfectly predicting $Y$ from $X$ is practically impossible in real-world scenarios. This is in part due to the fact that the joint distribution of $(X, Y)$ is \emph{unknown} in practice. Hence, we will frequently be making errors. To quantify how far off our predictions are, we need a \emph{loss function}
\[
    \highlightMath{
        l \colon \mathcal{Y} \times \mathcal{Y} \to \R
    }
\]
that let's us compare our prediction $f(x)$ with the true value $y$ by evaluating $l(f(x), y)$. This works great for individual observations $(x, y) \in \mathcal{X} \times \mathcal{Y}$. However, most of the time, we are interested in the \emph{expected error} or \emph{risk}
\[
    \highlightMath{
        L(f) = \Exp[l(f(X), Y)]
    }
\]
of a function $f \colon \mathcal{X} \to \mathcal{Y}$ for a pair of random variables $(X, Y)$. Clearly, $L(f)$ depends on the joint distribution of $X$ and $Y$ mentioned before. Once again, this joint distribution is unknown in practice. For this very reason, a large part of this course is devoted to finding \emph{distribution-free} upper bounds\footnote{i.e., bounds that are free of specific assumptions about the underlying probability distribution} of $L(f)$. In Chapter \ref{ch: binary classification}, we will see that this takes considerable effort, even for seemingly simple tasks like binary classification.

Taking $\mathcal{Y} = \R^d$, one particularly popular choice of loss function in machine learning is the \emph{squared error loss}\footnote{$\norm{\cdot}_2$ denotes the Euclidean norm on $\R^d$}
\[
    l(y_1, y_2) = \norm{y_1 - y_2}_2^2,
\]
whose associated expected error is the well-known mean squared error (MSE)
\[
    \MSE(f, Y) = \MSE(f) = \Exp[\norm{f(X) - Y}_2^2].
\]
The mean squared error is closely related to the conditional expectation $\Exp[Y \given X]$ of $Y$ given $X$. The latter, in turn, plays a central role in binary classification, the topic of the next chapter. Hence, let's take a closer look at the conditional expectation $\Exp[Y \given X]$.
