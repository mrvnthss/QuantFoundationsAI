%% SECTION 2.5 %%
\section{Learning with a Finite Dictionary}
\label{sec: learning with a finite dictionary}

Recall the setup of Section \ref{subsec: empirical classification}: given a family $\mathcal{H}$ of classifiers, we want to bound the estimation error $L(\hat{h}) - L(\bar{h})$, where
\[
    \hat{h} \in \argmin_{h \in \mathcal{H}} \hat{L}_n(h)
\]
is the empirical risk minimizer, which minimizes the \emph{empirical} risk over $\mathcal{H}$, and
\[
    \bar{h} \in \argmin_{h \in \mathcal{H}} L(h)
\]
is the oracle, which minimizes the \emph{true} risk over the family $\mathcal{H}$. Keep in mind that the empirical risk minimizer $\hat{h}$ is a random variable that depends on the random data $\mathcal{D}_n$. In this section, we will prove a bound on the estimation error $L(\hat{h}) - L(\bar{h})$ under the assumption that the family $\mathcal{H} = \set{h_1, \dots, h_M}$ is a \emph{finite} set of classifiers. In particular, we want to better understand the scaling of the estimation error with respect to the number $M$ of classifiers and the number $n$ of observations.

The starting point of this endeavor is Vapnik's lemma, named after Russian mathematician Vladimir Vapnik.

\begin{lemma}[Vapnik's Lemma]
\label{lem: bound on estimation error}
For any family of classifiers $\mathcal{H}$, the estimation error $L(\hat{h}) - L(\bar{h})$ is bounded by
\[
    L(\hat{h}) - L(\bar{h}) \leq 2 \sup_{h \in \mathcal{H}} \abs*{\hat{L}_n(h) - L(h)}.
\]
\end{lemma}

\begin{proof}
Clearly, the estimation error satisfies
\[
    L(\hat{h}) - L(\bar{h}) = L(\hat{h}) - {\color{blue} \hat{L}_n(\bar{h})} + \hat{L}_n(\bar{h}) - L(\bar{h}) \leq L(\hat{h}) - {\color{red} \hat{L}_n(\hat{h})} + \hat{L}_n(\bar{h}) - L(\bar{h}),
\]
since the inequality ${\color{red} \hat{L}_n(\hat{h})} \leq {\color{blue} \hat{L}_n(\bar{h})}$ holds by the very definition of the empirical risk minimzer. The result follows from applying a rather crude bound to the RHS of the inequality above:
\begin{align*}
    L(\hat{h}) - L(\bar{h}) &\leq L(\hat{h}) - \hat{L}_n(\hat{h}) + \hat{L}_n(\bar{h}) - L(\bar{h}) \\
    &\leq \abs*{\hat{L}_n(\hat{h}) - L(\hat{h})} + \abs*{\hat{L}_n(\bar{h}) - L(\bar{h})} \leq 2 \sup_{h \in \mathcal{H}} \abs*{\hat{L}_n(h) - L(h)}. \qedhere
\end{align*}
\end{proof}

Hence, in order to find a bound of the estimation error of a class $\mathcal{H}$, it seems promising to find bounds of the discrepancy between the empirical risk $\hat{L}_n(h)$ and the true risk $L(h)$ of classifiers $h$ in $\mathcal{H}$. A first result is the following:

\begin{theorem}
\label{thm: empirical vs true risk}
\begin{enumerate}
    \item For a classifier $h \colon \mathcal{X} \to \set{0, 1}$ and a confidence level $\delta > 0$,
    \[
        \P\left(\abs*{\hat{L}_n(h) - L(h)} < \sqrt{\frac{\log(2 / \delta)}{2n}}\right) \geq 1 - \delta.
    \]

    \item If $\mathcal{H} = \set{h_1, \dots, h_M}$ is a \emph{finite} family of $M$ classifiers, then
    \[
        \P\left(\max_{h \in \mathcal{H}} \abs*{\hat{L}_n(h) - L(h)} < \sqrt{\frac{\log(2M / \delta)}{2n}}\right) \geq 1 - \delta.
    \]
\end{enumerate}
\end{theorem}

\begin{proof}
Since $(X_i, Y_i)$ are i.i.d.\ draws from $\P_{(X, Y)}$, we have
\[
    \P(h(X) \neq Y) = \Exp[\indSet{h(X) \neq Y}] = \frac{1}{n} \sum_{i=1}^n \Exp[\indSet{h(X) \neq Y}] = \frac{1}{n} \sum_{i=1}^n \Exp[\indSet{h(X_i) \neq Y_i}],
\]
and hence,
\[
    \hat{L}_n(h) - L(h) = \frac{1}{n} \sum_{i=1}^n \indSet{h(X_i) \neq Y_i} - \Exp[\indSet{h(X_i) \neq Y_i}].
\]
Consequently, the first assertion follows from Hoeffding's inequality applied to $\indSet{h(X_i) \neq Y_i}$, with $i = 1, \dots, n$, and $t = \sqrt{\frac{\log(2 / \delta)}{2n}}$, since $\P(\abs{\ldots} < t) = 1 - \P(\abs{\ldots} \geq t) \geq 1 - 2\e^{-2nt^2} = 1 - \delta$.

For the second assertion, observe that, for $t = \sqrt{\frac{\log(2M/\delta)}{2n}} \,$,
\[
    \set{\max_{h \in \mathcal{H}} \abs*{\hat{L}_n(h) - L(h)} \geq t} = \bigcup_{j=1}^M \{\abs*{\hat{L}_n(h_j) - L(h_j)} \geq t\}
\]
and hence,
\[
    \P(\max_{h \in \mathcal{H}} \abs*{\hat{L}_n(h) - L(h)} < t) = 1 - \P(\max_{h \in \mathcal{H}} \abs*{\hat{L}_n(h) - L(h)} \geq t) \geq 1 - \sum_{j=1}^M \P(\abs*{\hat{L}_n(h_j) - L(h_j)} \geq t)
\]
by applying a simple union bound. As before, Hoeffding's inequality yields
\[
    \P(\abs*{\hat{L}_n(h_j) - L(h_j)} \geq t) \leq 2 \e^{-2nt^2} = \frac{\delta}{M}, \quad j = 1, \dots, M.
\]
Putting everything together, we obtain
\[
    \P(\max_{h \in \mathcal{H}} \abs*{\hat{L}_n(h) - L(h)} < t) \geq 1 - \sum_{j=1}^M \P(\abs*{\hat{L}_n(h_j) - L(h_j)} \geq t) \geq 1 - \sum_{j=1}^M \frac{\delta}{M} = 1 - \delta. \qedhere
\]
\end{proof}

We can now use this intermediate result to obtain a bound on the estimation error $L(\hat{h}) - L(\bar{h})$. Loosely speaking, while the empirical risk minimizer $\hat{h}$ generally performs worse (in terms of the true risk) than the oracle $\bar{h}$, their difference is not too big, given that the sample size $n$ is large enough compared to the number $M$ of classifiers in $\mathcal{H}$.

\begin{theorem}
\label{thm: estimation error finite dictionary}
For a finite family of classifiers $\mathcal{H} = \set{h_1, \dots, h_M}$ and a confidence level $\delta > 0$, the estimation error $L(\hat{h}) - L(\bar{h})$ satisfies
\[
    \P\left(L(\hat{h}) - L(\bar{h}) < \sqrt{\frac{2 \log(2M / \delta)}{n}}\right) \geq 1 - \delta.
\]
\end{theorem}

\begin{proof}
From Vapnik's lemma (Lemma \ref{lem: bound on estimation error}), we know that
\[
    L(\hat{h}) - L(\bar{h}) \leq 2 \sup_{h \in \mathcal{H}} \abs*{\hat{L}_n(h) - L(h)} = 2 \max_{1 \leq j \leq M} \abs*{\hat{L}_n(h_j) - L(h_j)}.
\]
Hence, Theorem \ref{thm: empirical vs true risk} implies
\begin{align*}
    \P\left(L(\hat{h}) - L(\bar{h}) < \sqrt{\frac{2 \log(2M / \delta)}{n}}\right) &\geq \P\left(2 \max_{1 \leq j \leq M} \abs*{\hat{L}_n(h_j) - L(h_j)} < \sqrt{\frac{2 \log(2M / \delta)}{n}}\right) \\[4pt]
    &= \P\left(\max_{1 \leq j \leq M} \abs*{\hat{L}_n(h_j) - L(h_j)} < \sqrt{\frac{\log(2M / \delta)}{2n}}\right) \geq 1 - \delta. \qedhere
\end{align*}
\end{proof}

Let us make a few remarks to close this chapter. First, the parameter $\delta > 0$ allows us to control the confidence we want to have in the bound on the estimation error. Here, we see a typical trade-off: for smaller $\delta$, the bound on the estimation error becomes less restrictive, but at the same time, the probability that the bound actually holds, increases (and vice versa). Further, for fixed $\delta$, the bound becomes sharper as the number $n$ of observations increases and the number $M$ of classifiers decreases.
