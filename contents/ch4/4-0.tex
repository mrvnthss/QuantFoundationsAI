%% CHAPTER 4 %%
\chapter{Vapnik-Chervonenkis (VC) Theory}
\label{ch: VC theory}

Once again, recall the general setup of Section \ref{subsec: empirical classification}: given a family $\mathcal{H}$ of classifiers, we can express the excess risk of the empirical risk minimizer $\hat{h}$ as
\[
    R(\hat{h}) = \underbrace{L(\hat{h}) - L(\bar{h})}_{\substack{\text{estimation} \\ \text{error}}} + \underbrace{L(\bar{h}) - L^*}_{\substack{\text{approximation} \\ \text{error}}}
\]
where $\bar{h} \in \mathcal{H}$ is the oracle, i.e., the classifier in $\mathcal{H}$ that minimizes the true risk over $\mathcal{H}$. The second term on the RHS is the approximation error, which remains fixed once we have settled on a family $\mathcal{H}$. Hence, we try to find bounds of the first term of the RHS, the estimation error. In Theorem \ref{thm: estimation error finite dictionary}, we had seen that the inequality
\[
    L(\hat{h}) - L(\bar{h}) < \sqrt{\frac{2 \log(2M / \delta)}{n}}
\]
holds with probability at least $1 - \delta$, if $\mathcal{H}$ is a \emph{finite} family of $M$ classifiers. Note that this upper bound for finite $\mathcal{H}$ cannot simply be extended to the case that $\mathcal{H}$ is \emph{infinite}. Essentially, to extend our previous results to the infinite case, we would have to show that only finitely many elements in a possibly infinite dictionary $\mathcal{H}$ \say{really matter}. This exactly is the goal of the theory developed by Russian mathematicians Vladimir Vapnik and Alexey Chervonenkis from 1960 to 1990.

We start with the unrealistic case that our family $\mathcal{H}$ of classifiers is finite and that the oracle $\bar{h} \in \mathcal{H}$ has zero error probability. In that case, the empirical risk minimizer $\hat{h}$ satisfies $\hat{L}_n(\hat{h}) = 0$ almost surely, which can be seen as follows: Since $\hat{h}$ is the classifier in $\mathcal{H}$ minimizing the empirical risk, we have
\[
    \Exp[\hat{L}_n(\hat{h})] = \Exp[\min_{h \in \mathcal{H}} \hat{L}_n(h)] \leq \min_{h \in \mathcal{H}} \Exp[\hat{L}_n(h)] = \min_{h \in \mathcal{H}} L(h) = L(\bar{h}) = 0.
\]
Because $\hat{L}_n(\hat{h})$ is non-negative, this implies that $\Prob(\hat{L}_n(\hat{h}) = 0) = 1$. We can also bound the \emph{true} error of the ERM as follows:

\begin{theorem}[Vapnik and Chervonenkis, 1974]
Let $\mathcal{H}$ be finite and assume $L(\bar{h}) = \min_{h \in \mathcal{H}} L(h) = 0$. Then, for every $n \in \N$, the empirical risk minimzer $\hat{h}$ satisfies
\[
    \Prob(L(\hat{h}) > \varepsilon) \leq \abs{\mathcal{H}} \e^{-n \varepsilon}, \quad \varepsilon > 0
\]
and
\[
    \Exp[L(\hat{h})] \leq \frac{1 + \log(\abs{\mathcal{H}})}{n}.
\]
\end{theorem}

\begin{proof}
Since $\hat{L}_n(\hat{h}) = 0$ almost surely, we have $\hat{h} \in \mathcal{H}_0 = \{h \in \mathcal{H} \with \hat L_n(h) = 0 \text{ a.s.}\}$, and hence
\begin{align*}
    \Prob(L(\hat{h}) > \varepsilon) &\leq \Prob\left(\max_{h \in \mathcal{H}_0} L(h) > \varepsilon\right) = \Exp\left[\ind{\{\max_{h \in \mathcal{H}_0} L(h) > \varepsilon\}}\right] \\[4pt]
    &=\Exp\left[\max_{h \in \mathcal{H}} \ind{\{\hat{L}_n(h) = 0\}} \indSet{L(h) > \varepsilon}\right] \leq \sum_{h \in \mathcal{H}, \, L(h) > \varepsilon} \Prob(\hat{L}_n(h) = 0).
\end{align*}
Since the probability $\Prob(\hat{L}_n(h) = 0)$ that no $(X_i, Y_i)$ falls in the set $\set{(x, y) \with h(x) \neq y}$ is at most $(1 - \varepsilon)^n$ if the probability of this set\footnote{This is precisely the true loss $L(h) = \Prob(h(X) \neq Y)$, which is larger than $\varepsilon$ by assumption.} is larger than $\varepsilon$, we conclude
\[
    \Prob(L(\hat{h}_n) > \varepsilon) \leq \sum_{h \in \mathcal{H}, \, L(h) > \varepsilon} \Prob(\hat{L}_n(h) = 0) \leq \abs{\mathcal{H}} (1 - \varepsilon)^n \leq \abs{\mathcal{H}} \e^{-n \varepsilon}.
\]

To bound the expected error probability of $\hat{h}$ for $n \in \N$, first recall that the expected value of a non-negative random variable $Z \geq 0$ can be computed as follows:
\[
    \Exp[Z] = \int_0^{\infty} 1 - F_Z(t) \dt = \int_0^{\infty} \Prob(Z > t) \dt.
\]
Hence, we have
\begin{align*}
    \Exp[L(\hat{h})] &= \int_0^{\infty} \Prob(L(\hat{h}) > t) \dt \\
    &\leq u + \int_u^{\infty} \Prob(L(\hat{h}) > t) \dt \leq u + \abs{\mathcal{H}} \int_u^{\infty} \e^{-nt} \dt = u + \abs{\mathcal{H}} \frac{\e^{-nu}}{n}.
\end{align*}
Since $u > 0$ was arbitrary, we can minimize the RHS to obtain the optimal solution $u^* = \log(\abs{\mathcal{H}}) / n$, giving the desired result.
\end{proof}
