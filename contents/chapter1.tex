%% PART I %%
\part{Statistical Learning Theory}

%% CHAPTER 1 %%
\chapter{Formal Setting}
\label{ch: formal setting}

In this first chapter, we briefly describe the formal setting that will accompany us for the remainder of this course. We will introduce some common machine learning terminology and we will fix some notation.

Throughout this course, we let $(X, Y)$ denote a pair of random variables defined on a common probability space $(\Omega, \mathcal{A}, \P)$, taking values in arbitrary spaces $(\mathcal{X}, \mathcal{F}_{\mathcal{X}})$ and $(\mathcal{Y}, \mathcal{F}_{\mathcal{Y}})$, respectively\footnote{$\mathcal{F}_{\mathcal{X}}$ and $\mathcal{F}_{\mathcal{Y}}$ denote $\sigma$-algebras on $\mathcal{X}$ and $\mathcal{Y}$, respectively. We won't be concerned with measurability in this course, so these won't play an important role.}. Most often, these spaces will be the Euclidean space $\R^d$ of dimension $d$. Also, we may sometimes just write $Z$ for the pair $(X, Y)$. We denote the joint distribution of $X$ and $Y$ by $\P_{(X, Y)}$, i.e.,
\[
    \P_{(X, Y)}(A, B) = \Pr{X \in A, Y \in B}, \quad A \in \mathcal{F}_{\mathcal{X}}, \; B \in \mathcal{F}_{\mathcal{Y}}.
\]
Similarly, we write $\P_X$ and $\P_Y$ for the marginal distributions of $X$ and $Y$, respectively. We will frequently be working with (hypothetical) data in this course. To this end, we let $\mathcal{D}_n = \set{(X_1, Y_1), \dots, (X_n, Y_n)}$ denote a set of $n$ pairs $Z_i = (X_i, Y_i)$, all of which are i.i.d.\ draws from the joint distribution of $(X, Y)$.
