%% SECTION 2.5 %%
\section{Learning with a Finite Dictionary}

Recall the setup of Section \ref{subsec: empirical classification}: given a family $\mathcal{H}$ of classifiers, we want to bound the estimation error $L(\hat h) - L(\bar h)$, where
\[
    \hat h \in \argmin_{h \in \mathcal{H}} \hat L_n(h)
\]
is the empirical risk minimizer, which minimizes the \emph{empirical} risk over $\mathcal{H}$, and
\[
    \bar h \in \argmin_{h \in \mathcal{H}} L(h)
\]
is the oracle, which minimizes the \emph{true} risk over the family $\mathcal{H}$. Keep in mind that the empirical risk minimizer $\hat h$ is a random variable that depends on the random data $\mathcal{D}_n$. In this section, we will prove a bound on the estimation error $L(\hat h) - L(\bar h)$ under the assumption that the family $\mathcal{H} = \set{h_1, \dots, h_M}$ is a \emph{finite} set of classifiers. In particular, we want to better understand the scaling of the estimation error with respect to the number $M$ of classifiers and the number $n$ of observations.

To get started, observe that the estimation error satisfies
\[
    L(\hat h) - L(\bar h) = \left(L(\hat h) - \hat L_n(\bar h)\right) + \left(\hat L_n(\bar h) - L(\bar h)\right) \leq \left(L(\hat h) - \hat L_n(\hat h)\right) + \left(\hat L_n(\bar h) - L(\bar h)\right),
\]
since $\hat L_n(\hat h) \leq \hat L_n(\bar h)$ by definition of the empirical risk minimzer. Thus, we can find a bound on the estimation error by bounding the deviation of the empirical risk $\hat L_n(h)$ of a classifier $h$ from its true risk $L(h)$. This is where Hoeffding's inequality comes in handy.

\begin{theorem}
\label{thm: empirical vs true risk}
\begin{enumerate}[(i)]
    \item For a classifier $h$ and $\delta > 0$,
    \[
        \Pr{\bigAbs{ \hat L_n(h) - L(h) } < \sqrt{\frac{\log(2/\delta)}{2n}}} \geq 1 - \delta.
    \]

    \item If $\mathcal{H} = \set{h_1, \dots, h_M}$ is a \emph{finite} family of $M$ classifiers, then
    \[
        \Pr{\max_{h \in \mathcal{H}} \bigAbs{ \hat L_n(h) - L(h) } < \sqrt{\frac{\log(2M/\delta)}{2n}}} \geq 1 - \delta.
    \]
\end{enumerate}
\end{theorem}

\begin{proof}
Since $(X_i, Y_i)$ are i.i.d. draws from $\P_{(X, Y)}$, we have
\[
    \Pr{h(X) \neq Y} = \Ex{\indSet{h(X) \neq Y}} = \frac{1}{n} \sum_{i=1}^n \Ex{\indSet{h(X) \neq Y}} = \frac{1}{n} \sum_{i=1}^n \Ex{\indSet{h(X_i) \neq Y_i}},
\]
and hence,
\[
    \hat L_n(h) - L(h) = \frac{1}{n} \sum_{i=1}^n \indSet{h(X_i) \neq Y_i} - \Ex{\indSet{h(X_i) \neq Y_i}}.
\]
The first assertion thus follows from Hoeffding's inequality applied to the random variables $\indSet{h(X_i) \neq Y_i}$, $i = 1, \dots, n$ and $t = \sqrt{\frac{\log(2/\delta)}{2n}}$, since $\Pr{\abs{\ldots} < t} = 1 - \Pr{\abs{\ldots} \geq t} \geq 1 - 2\e^{-2nt^2} = 1 - \delta$.

For the second assertion, observe that, for $t = \sqrt{\frac{\log(2M/\delta)}{2n}}$,
\[
    \set{\max_{h \in \mathcal{H}} \bigAbs{ \hat L_n(h) - L(h) } \geq t} = \bigcup_{j=1}^M \set{\bigAbs{ \hat L_n(h_j) - L(h_j) } \geq t}
\]
and hence,
\[
    \Pr{\max_{h \in \mathcal{H}} \bigAbs{ \hat L_n(h) - L(h) } < t} = 1 - \Pr{\max_{h \in \mathcal{H}} \bigAbs{ \hat L_n(h) - L(h) } \geq t} \geq 1 - \sum_{j=1}^M \Pr{\bigAbs{ \hat L_n(h_j) - L(h_j) } \geq t}
\]
by applying a simple union bound. As before, Hoeffding's inequality yields
\[
    \Pr{\bigAbs{ \hat L_n(h_j) - L(h_j) } \geq t} \leq 2\e^{-2nt^2} = \frac{\delta}{M}, \quad j = 1, \dots, M.
\]
Putting everything together, we obtain
\[
    \Pr{\max_{h \in \mathcal{H}} \bigAbs{ \hat L_n(h) - L(h) } < t} \geq 1 - \sum_{j=1}^M \Pr{\bigAbs{ \hat L_n(h_j) - L(h_j) } \geq t} \geq 1 - \sum_{j=1}^M \frac{\delta}{M} = 1 - \delta.
\]
\end{proof}

As hinted at before, we can now use this intermediate result to obtain a bound on the estimation error $L(\hat h) - L(\bar h)$. Loosely speaking, while the empirical risk minimizer $\hat h$ generally performs worse (in terms of the true risk) than the oracle $\bar h$, their difference is not too big, given that the sample size $n$ is large enough compared to the number $M$ of classifiers in $\mathcal{H}$.

\begin{theorem}
\label{thm: estimation error finite dictionary}
For a finite family of classifiers $\mathcal{H} = \set{h_1, \dots, h_M}$ and $\delta > 0$, the estimation error $L(\hat h) - L(\bar h)$ satisfies
\[
    \Pr{L(\hat h) - L(\bar h) < \sqrt{\frac{2\log(2M/\delta)}{n}}} \geq 1 - \delta.
\]
\end{theorem}

\begin{proof}
We have
\[
    L(\hat h) - L(\bar h) = \left(L(\hat h) - \hat L_n(\bar h)\right) + \left(\hat L_n(\bar h) - L(\bar h)\right) \leq \left(L(\hat h) - \hat L_n(\hat h)\right) + \left(\hat L_n(\bar h) - L(\bar h)\right),
\]
since $\hat L_n(\hat h) \leq \hat L_n(\bar h)$ by definition of the empirical risk minimzer. Since we do not have any information about the oracle $\bar h$, we bound both terms of the RHS by a maximum over $\mathcal{H}$:
\[
    \left(L(\hat h) - \hat L_n(\hat h)\right) + \left(\hat L_n(\bar h) - L(\bar h)\right) \leq 2 \max_{j} \bigAbs{ \hat L_n(h_j) - L(h_j) }.
\]
Summarizing, we have
\[
    L(\hat h) - L(\bar h) \leq 2 \max_j \bigAbs{ \hat L_n(h_j) - L(h_j) },
\]
and thus, Theorem \ref{thm: empirical vs true risk} implies
\begin{align*}
    \Pr{L(\hat h) - L(\bar h) < \sqrt{\frac{2\log(2M/\delta)}{n}}} &\geq \Pr{2 \max_j \bigAbs{ \hat L_n(h_j) - L(h_j) } < \sqrt{\frac{2\log(2M/\delta)}{n}}} \\
        &= \Pr{\max_j \bigAbs{ \hat L_n(h_j) - L(h_j) } < \sqrt{\frac{\log(2M/\delta)}{2n}}} \geq 1 - \delta.
\end{align*}
\end{proof}

Let us make a few remarks before we move on. First, the parameter $\delta > 0$ allows us to control the confidence we want to have in the bound on the estimation error. Here, we see a typical trade-off: for smaller $\delta$, the bound on the estimation error becomes less restrictive, but at the same time, the probability that the bound actually holds, increases (and vice versa). Further, for fixed $\delta$, the bound becomes sharper as the number $n$ of observations increases and the number $M$ of classifiers decreases.
